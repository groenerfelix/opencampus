---
title: "Estimation of Neural Net"
output: html_notebook
---

### Installation von Python und TensorFlow (nur einmalig nötig)

```{r}
# install.packages("reticulate")
# library(reticulate)
# 
# # Installation von miniconda (falls nicht vorhanden)
# install_miniconda()
# 
# # Anlegen einer speziellen Python Umgebung
# conda_create("r-reticulate")
# 
# # Installieren der Pakete in der angelegten Umgebung
# conda_install("r-reticulate", "pandas")
# conda_install("r-reticulate", "numpy")
# conda_install("r-reticulate", "tensorflow")
# 
# # Verwenden der speziellen Python Umgebung die zuvor erstellt wurde
# use_condaenv("r-reticulate")

```


### Aufruf des Skripts zur Datenaufbereitung
```{r}
source("neural-net-data-preparation.R")

```


### Laden benötigter  Packages
```{r}
library(reticulate)
library(ggplot2)
library(Metrics)

```


### Definition des Neuronalen Netzes
```{python}
# Benoetigte Python Libraries einbinden
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Definition der Form des tiefen neuronalen Netzes (Deep Neural Nets)
model = keras.Sequential([
  layers.Dense(29, activation='relu', input_shape=[len(r.train_dataset.keys())]),
  layers.Dense(19, activation='relu'),
  layers.Dense(13, activation='relu'),
  layers.Dense(1)
])


# Definition der Kosten-(Loss-)Funktion und der Optimierungsfunktion mit seinen Hyperparametern
model.compile(loss="mse",
              optimizer=tf.keras.optimizers.SGD(lr=0.00006, momentum=0.9, nesterov=True))
              


# Ausgabe einer Zusammenfassung zur Form des Modells, das geschaetzt wird (nicht notwendig)
model.summary()


```
```{r}
#### Erklärung zum Neuronalen Netz
## Hidden layer und Neuronen 

# input layer: 10 features, 29 input variablen 29/19/13/1
# 1 hidden layer wenn Daten noch clusterbar sind
# 2 hidden layer bei komplexeren Datensätzen
# 2 hidden layer haben sich awohl als oft nützlich erwiesen
# mehr als zwei hidden layer unnötig für Problem, führt zu overfitting
# Faustregel: 2/3 von einem zum nächsten layer

## hParamenter von SGD
# start values: Generally used values lr = 1e-4, momentum=0.9.
# zuerst an lr rumgespielt, weil irgendwo gelesen das wichtiger
# danach momentum optimiert, d.h also die Grenzen abgetastet und die Mitte genommen

# nesterov: https://dominikschmidt.xyz/nesterov-momentum/
# next steps: optimizer wie adam und lr feedback loop wäre nice, bin gescheitert... 


## Quellen
# https://www.researchgate.net/post/How_do_I_represent_input_variables_for_artificial_neural_network_design 
# https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e
# https://stackoverflow.com/questions/51017181/what-should-be-the-value-for-learning-rate-and-momentum
# https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/

```

### Schätzung des neuronalen Netzes

```{python}
# Schaetzung des Modells
history = model.fit(r.train_dataset, r.train_labels, epochs=1500, validation_data = (r.test_dataset, r.test_labels), verbose=0)

# Ggf. Speichern des geschaetzten Modells
model.save("python_model_final_presentation_1500.h5")

```



### Auswertung der Modelloptimierung
```{r}
# Grafische Ausgabe der Modelloptimierung

# create data
data <- data.frame(val_loss = unlist(py$history$history$val_loss),
                  loss = unlist(py$history$history$loss))

# Plot
ggplot(data[-1,]) +
  geom_line( aes(x=1:length(val_loss), y=val_loss, colour = "Validation Loss" )) +
  geom_line( aes(x=1:length(loss), y=loss, colour = "Training Loss" )) +
  scale_colour_manual( values = c("Training Loss"="blue", "Validation Loss"="red") ) +
  labs(title="Loss Function Values During Optimization") +
  xlab("Iteration Number") +
  ylab("Loss") +
  theme_minimal()

```


### (Ggf.) Laden eines gespeicherten Neuronalen Netzes ###
```{python}
# model = keras.models.load_model("python_model_best.h5")

```


### Auswertung der Schätzergebnisse ###
```{r}
# Schätzung der (normierten) Preise für die Trainings- und Testdaten
train_predictions_norm <- py$model$predict(train_dataset)
test_predictions_norm <- py$model$predict(test_dataset)

# Rückberechnung der normierten Preisschätzungen zu den tatsächlichen Preisschätzungen bzw. Preisen
train_predictions <- (train_predictions_norm * norm_values_list$sd[1] ) + norm_values_list$mean[1]
test_predictions <- (test_predictions_norm * norm_values_list$sd[1]) + norm_values_list$mean[1]
# Selektion der zugehörigen tatsächlichen Preise
train_actuals <- umsatzdaten$Umsatz[train_ind]
test_actuals <- umsatzdaten$Umsatz[-train_ind]


# Vergleich der Gütekriterien für die Traingings- und Testdaten
cat(paste0("MAPE on the Training Data:\t", format(mape(train_predictions, train_actuals)*100, digits=3, nsmall=2)))
cat(paste0("\nMAPE on the Test Data:\t\t", format(mape(test_predictions, test_actuals)*100, digits=3, nsmall=2)))

# Vergleich mit MAE
cat(paste0("\nMAE on the Training Data:\t", format(mae(train_actuals, train_predictions)*100, digits=3, nsmall=2)))
cat(paste0("\nMAE on the Test Data:\t\t", format(mae(test_actuals, test_predictions)*100, digits=3, nsmall=2)))


```

```{r}

## Grafischer vergleich der vorhergesagten und der tatsächlichen Preise für die Trainings- und Testdaten

# Zusammenstellung der Daten für die Plots
data_train <- data.frame(prediction = train_predictions/1000, actual = train_actuals/1000)
data_test <- data.frame(prediction = test_predictions/1000, actual = test_actuals/1000)

# Plot der Ergebnisse der Trainingsdaten
ggplot(data_train[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Training Data") +
  xlab("Case Number") +
  ylab("Sales in 1.000 USD") +
  theme_minimal()

# Plot der Ergebnisse der Testdaten
ggplot(data_test[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Test Data") +
  xlab("Case Number") +
  ylab("Sales in 1.000 USD") +
  theme_minimal()


```

```{r}
# Vorhersage für einen einzelnen Fall
cat(paste0("Vorergesagter Preis:\t", format(test_predictions[100], digits=2, nsmall =0)))
cat(paste0("\nTatsächlicher Preis:\t", test_actuals[100]))


```
